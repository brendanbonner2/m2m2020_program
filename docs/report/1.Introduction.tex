\section{Introduction}

The topic covers the recording of CNN layer metrics during the training
sequence. The hypothesis is to examine trust mechanisms for explaining AI, and
providing an immutable record of the training sequences prior to a model being
utilised or customised.

The practicum focuses on two approaches to this;
recording and visualisation of the sequence of changes during the training of a
model. During model fitting at configurable intervals, layer characteristics are
measured and recorded, alongside mathematical verification, and stored. This can
be visualised, supporting modern explainable AI measures, to verify model
behaviour according to training, and can be trusted.

The proposal builds upon the attempts to provide additional metrics to support
the establishment of the concept of fairness in AI. In comparison with human
trust, which is established through verification and measurement of academic and
professional credentials, Fairness in AI is an addendum to the model being
examined, with attempts to quantify via measures such as Thiel scores for
certain bias tests. This is further developed by IBMs AI360 Fairness toolkit
paper which show how this is measured and corrected, but not being able to
identify where that behaviour was acquired and why a model is trained in a
certain way to develop these outcomes.
 There is currently a gap in identifying
if there is a provenance of trust when using deployed AI systems, and if sibling
and inherited models retain these behaviours and if this can be visualised.
While attempts to visualise the internals of decisions in the Simonyon and Bau
papers, these focus on trying to visualise the pathways being activated in a 2
dimensional map. The dissection and visualisation papers provide a key how to
measure and record this aspect of models, and our research will examine if there
is a potential to securely record neural network snapshots.
 
 The question being asked is to determine if we develop a novel mechanism and metric for
representing the evolution of knowledge within a neural network during the
training and development stages of creating a model. By analysing the state of
the art in terms of AI fairness and methods of visualising the dissected layers
common networks, the research will establish if creating an immutable chain of
layer delta changes over time will provide an insight into the trustworthiness
of the underlying model.

 The second part of the research will ask if the
information being stored can be visualised based on the elements of explainable
AI to be able to identify which iteration and epoch of the training process is
responsible for the manifestation of the behaviour. Ultimately, the outcome of
the research should ask if providing verifiable additional data on how an
artificial intelligence system evolves over time, if this can provide a
foundation into further research towards comparisons of human trust and
artificial intelligence trust.

\subsection{Synopsis}
To deliver the model synopsis, a reduction algorithm was applied to each layer in the model, reducing the weights and biases to a pair of unique identifiers containing the \textit{standard deviation} and the \textit{variance}, representing the distribution skew of the data away from a normal distribution. Once this synopsis is applied, we were able to reduce all default keras models from several megabytes to several kb arrays that are comparabile and the basis for generation of a unique signature.

Once the signature is generated, we are able to create a immutable link to a previous model, and store the deviation of both the structure and the weights as a factor. The key element of this is the generation of a lineage for both training and validating that there is a history of the model's evolution.