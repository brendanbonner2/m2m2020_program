\section{Introduction}
Within the field of machine learning, there is an issue surrounding the lack of transparency of artificial intelligence solutions that are being deployed in commercial and public domains. This has led to initiatives such as the EU developing a white paper on Trustworthy AI \cite{high-level_expert_group_on_ai_ethics_2019}, which has provided an overview of the trust level needed in AI in \cite{ryan_ai_2020}. The most difficult to ascertain is the AI technology itself.

Within the HLEG description, there are a number of references to the \textit{black box} of articial intelligence, where the complexity of a AI Neural Network system can no longer be analysed in a manner similar to statistical systems. Numerous attempts have been made to look inside the layers of the neural network to see what neurons are being activated \cite{kim_interpretability_2018}, and visualising where the attention is focused prior to making a prediction, but so far no provision has been made to establish trust in a system in a method closer to the human environment.

When a person visits a doctor, barrister or any expert, they develop trust because they can access their life experience that will give them the competence to provide the service. We can look up their professional and academic qualifications, review their professional and associative history, and use our udnerstanding that these provide the provenance of trust. It is with this analogy that we will investigate if we can establish a similar provenance of trust for AI systems. The aim is straightforward, can we create a \textit{linkedin} for software models that we can use to evaluate the creation, initial seeding, and then layers of training needed to improve the trust of AI systems. If we can look at the CV of a software model, we should be able to show which base model the system was built on (i.e.  ) and common training (i.e. on \cite{deng_imagenet_2009})

\subsection{Generating a Model Synopsis}
To deliver the model synopsis, a reduction algorithm was applied to each layer in the model, reducing the weights and biases to a pair of unique identifiers containing the \textit{standard deviation} and the \textit{variance}, representing the distribution skew of the data away from a normal distribution. Once this synopsis is applied, we were able to reduce all default keras models from several megabytes to several kb arrays that are comparabile and the basis for generation of a unique signature.

\subsection{Objectives}
The overall objectives of the practicum are as follows:

\begin{itemize}
    \item Discover a method to extract a unique signature from any Deep Neural Network software model.
    \item Create a local and global verification trail for
    \begin{itemize}
        \item A Path of provenance to all ancestor models
        \item A method to evaluate divergence of the child model from the parent
    \end{itemize}
    \item Validation of Provenance via verification and illustration
\end{itemize}

Once the signature is generated, we are able to create a immutable link to a previous model, and store the deviation of both the structure and the weights as a factor. The key element of this is the generation of a lineage for both training and validating that there is a history of the model's evolution.

\subsection{Introduction Simple}
Within the field of machine learning, there is an issue surrounding the lack of transparency of artificial intelligence solutions that are being deployed in commercial and public domains. This has led to initiatives such as the EU developing a white paper on Trustworthy AI \cite{high-level_expert_group_on_ai_ethics_2019}, which has provided an overview of the trust level needed in AI in \cite{ryan_ai_2020}. The most difficult to ascertain is the AI technology itself.

Within the HLEG description, there are a number of references to the \textit{black box} of articial intelligence, where the complexity of a AI Neural Network system can no longer be analysed in a manner similar to statistical systems. Numerous attempts have been made to look inside the layers of the neural network to see what neurons are being activated \cite{kim_interpretability_2018}, and visualising where the attention is focused prior to making a prediction, but so far no provision has been made to establish trust in a system in a method closer to the human environment.

When a person visits a doctor, barrister or any expert, they develop trust because they can access their life experience that will give them the competence to provide the service. We can look up their professional and academic qualifications, review their professional and associative history, and use our udnerstanding that these provide the provenance of trust. It is with this analogy that we will investigate if we can establish a similar provenance of trust for AI systems. The aim is straightforward, can we create a \textit{linkedin} for software models that we can use to evaluate the creation, initial seeding, and then layers of training needed to improve the trust of AI systems. If we can look at the CV of a software model, we should be able to show which base model the system was built on (i.e.  \cite{project_sherpa_httpswwwproject-sherpaeuethics-by-design_2019}) and common training (i.e. on \cite{deng_imagenet_2009})

The topic covers the recording of CNN layer metrics during the training
sequence. The hypothesis is to examine trust mechanisms for explaining AI, and
providing an immutable record of the training sequences prior to a model being
utilised or customised.

The practicum focuses on two approaches to this;
recording and visualisation of the sequence of changes during the training of a
model. During model fitting at configurable intervals, layer characteristics are
measured and recorded, alongside mathematical verification, and stored. This can
be visualised, supporting modern explainable AI measures, to verify model
behaviour according to training, and can be trusted.

The proposal builds upon the attempts to provide additional metrics to support
the establishment of the concept of fairness in AI. In comparison with human
trust, which is established through verification and measurement of academic and
professional credentials, Fairness in AI is an addendum to the model being
examined, with attempts to quantify via measures such as Thiel scores for
certain bias tests. This is further developed by IBMs AI360 Fairness toolkit
paper which show how this is measured and corrected, but not being able to
identify where that behaviour was acquired and why a model is trained in a
certain way to develop these outcomes.

There is currently a gap in identifying if there is a provenance of trust when using deployed AI systems, and if sibling
and inherited models retain these behaviours and if this can be visualised.
While attempts to visualise the internals of decisions in the Simonyon and Bau
papers, these focus on trying to visualise the pathways being activated in a 2
dimensional map. The dissection and visualisation papers provide a key how to
measure and record this aspect of models, and our research will examine if there
is a potential to securely record neural network snapshots.
 
The question being asked is to determine if we develop a novel mechanism and metric for
representing the evolution of knowledge within a neural network during the
training and development stages of creating a model. By analysing the state of
the art in terms of AI fairness and methods of visualising the dissected layers
common networks, the research will establish if creating an immutable chain of
layer delta changes over time will provide an insight into the trustworthiness
of the underlying model.

 The second part of the research will ask if the
information being stored can be visualised based on the elements of explainable
AI to be able to identify which iteration and epoch of the training process is
responsible for the manifestation of the behaviour. Ultimately, the outcome of
the research should ask if providing verifiable additional data on how an
artificial intelligence system evolves over time, if this can provide a
foundation into further research towards comparisons of human trust and
artificial intelligence trust.
