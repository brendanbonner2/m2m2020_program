\section{Introduction}
Artificial Intelligence has a trust issue. In all areas related to the field of machine learning, the absence of a perception of transparency within artificial intelligence across multiple domains highlight a lack of ethical oversight and visibility. This has led to guidelines such as the EU white paper on Trustworthy AI\cite{high-levelexpertgrouponaiEthicsGuidelinesTrustworthy2019}, which has initiated actions across the software ecosystem to put forward measures needed for trustworthy AI to be adapted, as discussed in Ryan\cite{ryanAIWeTrust2020}, and driving far reaching legislation in the forthcoming EU AI Act. The most striking observation across all discussions of AI, is that the most difficult component in the trust ecosystem to ascertain is the AI technology itself.

Artificial Intelligence research attributes this to the \textit{black box} problem\cite{adadiPeekingBlackBoxSurvey2018}, where the complexity of AI Neural Network systems can no longer be analysed thoroughly in a manner similar to statistical systems. Numerous attempts have been made to look inside the layers of the neural network to see what neurons are being activated\cite{kimInterpretabilityFeatureAttribution2018}, and visualising where the attention is focused prior to making a prediction, but so far no provision has been made to identify the required information to \textit{trust} the solution using AI in a method closer to human trust levels.

To understand how to achieve this trust, we need to equate parallels in the natural world. For instance, when a person visits a doctor, boards a bus or plane, or ask for financial advice; they develop trust due to a mixture of innate evolutionary biology unique to the user and insight. When exposed to non-trusting actors, they can access the credentials unique to the provider that reinforce of enhance \textit{trust} that the practitioners has the competence to provide the service. We can look up their professional and academic qualifications, review their associative history, and use our reasoning that these provide a basis for a provenance of trust.

It is with this analogy that we will investigate if we can establish a similar provenance of trust for AI systems. The aim is straightforward; can we create an architecture to support a \textit{Linkedin.com} for software models that we can use to evaluate the creation, initial seeding, and then incremental training needed to validate the trust of AI systems. If we can look at the \textit{Résumé} of a software model, we should be able to show which base model the system was built on (i.e. the majority of core vision applications are built using Keras models trained on ImageNet\cite{dengImageNetLargescaleHierarchical2009}), and the outcome that training the model had to influence the development of the model. This \textit{Practicum} will research how the addition of provenance to software models, the \textit{brain} of Artificial Intelligence, could be a solution to improving human levels of trust\cite{mcleodTrust2020} to machine learning.

We developed a cloud based solution to provide this, after examining current explainable AI (XAI) methods to visualise and identify internal changes in an AI model. Subsequent practical application created a system for extracting a model synopsis using a reduction algorithm applied to each layer across a model. The reduction of the weights and biases to a pair of unique identifiers containing the \textit{standard deviation} and the \textit{variance}, is then stored in a development trace for training, and a release database for deployment. Once this synopsis was developed, tested the capacity to reduce all default Keras models from several megabytes to several kilobyte arrays that are comparable, and the input for generation of a unique SHA-256 signature. The signature is a unique identifier which will create a single record from which we can identify ancestors, descendants and quickly show the delta between every relation.

The overall objectives of the \textit{Practicum} can be broken down into examining the state of the art, researching methods of generating an open recording solution for capturing AI model development and deployment, and focus on achieving this with the following objectives:

\begin{itemize}
    \item Discover a method to extract a unique signature from any Deep Neural Network software model.
    \item Create a local and global verification trail for a path of provenance to all ancestor models
    \item Create a local and global verification trail for a method to evaluate divergence of the child model from the parent
    \item Validation of Provenance via Verification and Illustration
\end{itemize}

Using a package of tools developed to deliver these objects, we created a repository of signatures and model impressions, built upon an immutable link to their parent. The capacity of generalising the lineage for both training and validating will be the basis for further research into exploration of a history of the model's evolution.