\section{Introduction}
Artificial Intelligence has a trust issue. In all areas related to the field of machine learning, the absence of transparency perceived within artificial intelligence solutions across domains highlight a lack of ethical oversight and visibility. This has led to initiatives such as the EU white paper on Trustworthy AI\cite{high-levelexpertgrouponaiEthicsGuidelinesTrustworthy2019}, which has initiated actions across the complete software ecosystem to put forward trust measures needed for AI to be adapted (i.e. Ryan\cite{ryanAIWeTrust2020}), and far reaching legislation in the forthcoming EU AI Act.

\noindent\rule[0.5ex]{\linewidth}{1pt}
The most difficult component in the trust ecosystem to ascertain, is the AI technology itself.
\noindent\rule[0.5ex]{\linewidth}{1pt}


Artificial Intelligence research attributes this to the \textit{black box} problem\cite{adadiPeekingBlackBoxSurvey2018}, where the complexity of AI Neural Network systems can no longer be analysed thoroughly in a manner similar to statistical systems. Numerous attempts have been made to look inside the layers of the neural network to see what neurons are being activated\cite{kimInterpretabilityFeatureAttribution2018}, and visualising where the attention is focused prior to making a prediction, but so far no provision has been made to identify how to \textit{trust} the solution using AI in a method closer to human trust levels.

To understand how to achieve this trust, we need to equate parallels or a person visits a doctor, boards a bus or plane, or ask for financial advice; they develop trust due to a core evolutionary biology unique to the user, or they can access their credentials unique to the provider that provide \textit{trust} they have competence to provide the service. We can look up their professional and academic qualifications, review their associative history, and use our reasoning that these provide a basis for a provenance of trust.

It is with this analogy that we will investigate if we can establish a similar provenance of trust for AI systems. The aim is straightforward; can we create an architecture to support a \textit{Linkedin.com} for software models that we can use to evaluate the creation, initial seeding, and then layers of training needed to validate the trust of AI systems. If we can look at the CV of a software model, we should be able to show which base model the system was built on and the outcome that training (i.e. the majority of core vision applications are build using Keras models trained on ImageNet\cite{dengImageNetLargescaleHierarchical2009}), the the influence it has on the development of the model. This \textit{Practicum} will research how the addition of provenance to software models, the \textit{brain} of Artificial Intelligence, could be a solution to improving human levels of trust\cite{mcleodTrust2020} to machine learning.

We developed a system after examining current explainable AI (XAI) methods to visualise and identify inside an AI model. Subsequent practical application created a system for extracting a model synopsis utilising a reduction algorithm applied to each layer across a model. The reduction of the weights and biases to a pair of unique identifiers containing the \textit{standard deviation} and the \textit{variance}, is then stored in a development and release database. Once this synopsis was developed, we were able to reduce all default Keras models from several megabytes to several kilobyte arrays that are comparable, and the basis for generation of a unique signature. The signature is a unique identifier within software development teams and upon release of a software model, this will create a single record from which we can identify ancestors, descendants and quickly show the delta between every relation.

The overall objectives of the \textit{Practicum} can be broken down into examining the state of the art, and then researching methods of generating an open recording solution for capturing AI model development and deployment. We completed this by researching the following fields:

\begin{itemize}
    \item Discover a method to extract a unique signature from any Deep Neural Network software model.
    \item Create a local and global verification trail for a path of provenance to all ancestor models
    \item Create a local and global verification trail for a method to evaluate divergence of the child model from the parent
    \item Validation of Provenance via Verification and Illustration
\end{itemize}

Using a package of tools developed to deliver these objects, we are able to create a repository of signatures and model impressions, built upon an immutable link to it parent. The capacity of generalising the lineage for both training and validating will be the basis for further research into exploration of a history of the model's evolution.