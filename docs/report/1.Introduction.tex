\section{Introduction}
Artificial Intelligence has a trust issue. Within the field of machine learning, there is an issue surrounding the lack of transparency of artificial intelligence solutions that are being deployed in commercial and public domains. This has led to initiatives such as the EU developing a white paper on Trustworthy AI\cite{high-level_expert_group_on_ai_ethics_2019}, which has been scrutinised across the complete Artificial Intelligence trust levels needed in AI in Ryan\cite{ryan_ai_2020}. The most difficult component in the trust ecosystem to ascertain is the AI technology itself.

Artificial Intelligences research attributes this to the \textit{black box} problem\cite{adadi_peeking_2018}, where the complexity of a AI Neural Network system can no longer be analysed in a manner similar to statistical systems. Numerous attempts have been made to look inside the layers of the neural network to see what neurons are being activated \cite{kim_interpretability_2018}, and visualising where the attention is focused prior to making a prediction, but so far no provision has been made to establish trust in a system in a method closer to the human environment.

When a person visits a doctor, barrister or any expert, they develop trust because they can access their life experience that will give them the competence to provide the service. We can look up their professional and academic qualifications, review their professional and associative history, and use our understanding that these provide the provenance of trust. It is with this analogy that we will investigate if we can establish a similar provenance of trust for AI systems. The aim is straightforward, can we create a \textit{linkedin} for software models that we can use to evaluate the creation, initial seeding, and then layers of training needed to improve the trust of AI systems. If we can look at the CV of a software model, we should be able to show which base model the system was built on (i.e. Project Sherpa\cite{project_sherpa_httpswwwproject-sherpaeuethics-by-design_2019}) and the outcome that training (i.e. on \cite{deng_imagenet_2009}) has on the development of the model. This practicum will research how the addition of provenance to software models, the \textit{brain} of Artificial Intelligence, could be a solution to improving human levels of trust\cite{mcleod_trust_2020} to machine learning.

\subsection{Generating a Model Synopsis}
We will provide this be examining current explainable AI (XAI) methods to visualise and identify inside an AI model. Based on the available information, the practical element will create a system for extracting a model synopsis utilising a reduction algorithm applied to each layer across a model This reduction of the weights and biases to a pair of unique identifiers containing the \textit{standard deviation} and the \textit{variance}, representing the distribution skew of the data away from a normal distribution. Once this synopsis is applied, we were able to reduce all default keras models from several megabytes to several kb arrays that are comparable and the basis for generation of a unique signature. The signature will be a unique identifier within software development teams and upon release of a model to the community, be a single record from which we can identify ancestors, descendants and quickly show the delta between the,

The overall objectives of the practicum can be broken down into examining the state of the art, and then researching methods of generating an open recording solution for capturing AI model development and deployment. We completed this by researching the following fields:

\begin{itemize}
    \item Discover a method to extract a unique signature from any Deep Neural Network software model.
    \item Create a local and global verification trail for
    \begin{itemize}
        \item A Path of provenance to all ancestor models
        \item A method to evaluate divergence of the child model from the parent
    \end{itemize}
    \item Validation of Provenance via Verification and Illustration
\end{itemize}

Once the signature is generated, we are able to create a immutable link to a previous model, and store the deviation of both the structure and the weights as a factor. The key element of this is the generation of a lineage for both training and validating that there is a history of the model's evolution.

