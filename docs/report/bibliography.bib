
@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black}}-{{Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black}}-{{Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms},
  file = {/Users/brendan/Zotero/storage/SFG45LMF/Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf}
}

@online{alainUnderstandingIntermediateLayers2018,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Yoshua},
  date = {2018-11-22},
  shortjournal = {ArXiv161001644 Cs Stat},
  eprint = {1610.01644},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1610.01644},
  urldate = {2021-01-31},
  abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brendan/Zotero/storage/Q8F7QYXJ/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf;/Users/brendan/Zotero/storage/KSV28M9G/1610.html}
}

@article{asaroAIEthicsPredictive2019,
  title = {{{AI Ethics}} in {{Predictive Policing}}: {{From Models}} of {{Threat}} to an {{Ethics}} of {{Care}}},
  shorttitle = {{{AI Ethics}} in {{Predictive Policing}}},
  author = {Asaro, Peter M.},
  date = {2019-06},
  journaltitle = {IEEE Technology and Society Magazine},
  shortjournal = {IEEE Technol. Soc. Mag.},
  volume = {38},
  number = {2},
  pages = {40--53},
  issn = {0278-0097, 1937-416X},
  doi = {10.1109/MTS.2019.2915154}
}

@online{bauNetworkDissectionQuantifying2017,
  title = {Network {{Dissection}}: {{Quantifying Interpretability}} of {{Deep Visual Representations}}},
  shorttitle = {Network {{Dissection}}},
  author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  date = {2017-04-19},
  shortjournal = {ArXiv170405796 Cs},
  eprint = {1704.05796},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1704.05796},
  urldate = {2021-02-06},
  abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,I.2.10},
  file = {/Users/brendan/Zotero/storage/WNNFCCH9/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf;/Users/brendan/Zotero/storage/XPD9ZHSX/1704.html}
}

@book{bebisAdvancesVisualComputing2015,
  title = {Advances in {{Visual Computing}}: 11th {{International Symposium}}, {{ISVC}} 2015, {{Las Vegas}}, {{NV}}, {{USA}}, {{December}} 14-16, 2015, {{Proceedings}}, {{Part I}}},
  shorttitle = {Advances in {{Visual Computing}}},
  editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Pavlidis, Ioannis and Feris, Rogerio and McGraw, Tim and Elendt, Mark and Kopper, Regis and Ragan, Eric and Ye, Zhao and Weber, Gunther},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9474},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-27857-5},
  isbn = {978-3-319-27856-8 978-3-319-27857-5},
  langid = {english}
}

@online{brownleeHowVisualizeFilters2019,
  title = {How to {{Visualize Filters}} and {{Feature Maps}} in {{Convolutional Neural Networks}}},
  author = {Brownlee, Jason},
  date = {2019-05-05T19:00:37+00:00},
  url = {https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/},
  urldate = {2021-08-10},
  abstract = {Deep learning neural networks are generally opaque, meaning that although they can make useful and skillful predictions, it is not […]},
  langid = {american},
  organization = {{Machine Learning Mastery}},
  file = {/Users/brendan/Zotero/storage/CZ8TUL6J/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/Users/brendan/Zotero/storage/6JTBJMDE/5206848.html}
}

@online{gatysNeuralAlgorithmArtistic2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2015-09-02},
  shortjournal = {ArXiv150806576 Cs Q-Bio},
  eprint = {1508.06576},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1508.06576},
  urldate = {2021-01-31},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/brendan/Zotero/storage/LTZEM5BE/Gatys et al. - 2015 - A Neural Algorithm of Artistic Style.pdf;/Users/brendan/Zotero/storage/MNBXMGKT/1508.html}
}

@online{genekoganWhatNeuralNetworks2021,
  title = {What {{Neural Networks See}} by {{Gene Kogan}} - {{Experiments}} with {{Google}}},
  author = {{Gene Kogan}},
  date = {2021},
  url = {https://experiments.withgoogle.com/what-neural-nets-see},
  urldate = {2021-08-09},
  abstract = {Since 2009, coders have created thousands of amazing experiments using Chrome, Android, AI, WebVR, AR and more. We're showcasing projects here, along with helpful tools and resources, to inspire others to create new experiments.},
  file = {/Users/brendan/Zotero/storage/ZWTT86YC/what-neural-nets-see.html}
}

@inproceedings{golleMachineLearningAttacks2008,
  title = {Machine Learning Attacks against the {{Asirra CAPTCHA}}},
  booktitle = {Proceedings of the 15th {{ACM}} Conference on {{Computer}} and Communications Security},
  author = {Golle, Philippe},
  date = {2008},
  pages = {535--542},
  file = {/Users/brendan/Zotero/storage/N38CZRPQ/1455770.html}
}

@incollection{hecht-nielsenIIITheoryBackpropagation1992,
  title = {{{III}}.3 - {{Theory}} of the {{Backpropagation Neural Network}}**{{Based}} on “Nonindent” by {{Robert Hecht}}-{{Nielsen}}, Which Appeared in {{Proceedings}} of the {{International Joint Conference}} on {{Neural Networks}} 1, 593–611, {{June}} 1989. © 1989 {{IEEE}}.},
  booktitle = {Neural {{Networks}} for {{Perception}}},
  author = {Hecht-nielsen, ROBERT},
  editor = {Wechsler, Harry},
  date = {1992-01-01},
  pages = {65--93},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-741252-8.50010-8},
  abstract = {This chapter presents a survey of the elementary theory of the basic backpropagation neural network architecture, covering the areas of architectural design, performance measurement, function approximation capability, and learning. The survey includes a formulation of the backpropagation neural network architecture to make it a valid neural network and a proof that the backpropagation mean squared error function exists and is differentiable. Also included in the survey is a theorem showing that any L2 function can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. An appendix presents a speculative neurophysiological model illustrating the way in which the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of cerebral cortex. One of the crucial decisions in the design of the backpropagation architecture is the selection of a sigmoidal activation function.},
  isbn = {978-0-12-741252-8},
  langid = {english},
  file = {/Users/brendan/Zotero/storage/3VIB5AJA/B9780127412528500108.html}
}

@article{heDeepResidualLearning2015a,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  url = {https://arxiv.org/abs/1512.03385v1},
  urldate = {2021-08-09},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  langid = {english},
  file = {/Users/brendan/Zotero/storage/N9WFKGC3/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/brendan/Zotero/storage/RMW4KVK3/1512.html}
}

@report{high-levelexpertgrouponaiEthicsGuidelinesTrustworthy2019,
  type = {Report},
  title = {Ethics Guidelines for Trustworthy {{AI}}},
  author = {{High-Level Expert Group on AI}},
  date = {2019-04},
  institution = {{European Commission}},
  location = {{Brussels}},
  url = {https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai},
  abstract = {The aim of the Guidelines is to promote Trustworthy AI. Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them.},
  langid = {english},
  keywords = {European_Commission artificial_intelligence digital_ethics discrimination ethics}
}

@article{jobinGlobalLandscapeAI2019,
  title = {The Global Landscape of {{AI}} Ethics Guidelines},
  author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  date = {2019-09},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {9},
  pages = {389--399},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0088-2},
  abstract = {In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.},
  issue = {9},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Ethics;Information systems and information technology;Information technology;Science, technology and society Subject\_term\_id: ethics;information-systems-and-information-technology;information-technology;science-technology-and-society},
  file = {/Users/brendan/Zotero/storage/BNH5DQ55/Jobin et al. - 2019 - The global landscape of AI ethics guidelines.pdf;/Users/brendan/Zotero/storage/3SAZAAA3/s42256-019-0088-2.html}
}

@article{kevinUnderstandingEthicsHuman2019,
  title = {Understanding {{Ethics}} and {{Human Rights}} in {{Smart Information Systems}}},
  author = {Kevin, Macnish and Mark, Ryan and Bernd, Stahl},
  date = {2019},
  journaltitle = {The ORBIT Journal},
  shortjournal = {The ORBIT Journal},
  volume = {2},
  number = {2},
  pages = {1--34},
  issn = {25158562},
  doi = {10.29297/orbit.v2i1.102},
  langid = {english},
  file = {/Users/brendan/Zotero/storage/2VHZXQ8W/Kevin et al. - 2019 - Understanding Ethics and Human Rights in Smart Inf.pdf}
}

@online{kimInterpretabilityFeatureAttribution2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  date = {2018-06-07},
  shortjournal = {ArXiv171111279 Stat},
  eprint = {1711.11279},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1711.11279},
  urldate = {2021-02-06},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/brendan/Zotero/storage/PP8222XT/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf;/Users/brendan/Zotero/storage/ZZBHYXX3/1711.html}
}

@article{kimOntologydrivenBlockchainDesign2018,
  title = {Toward an Ontology-Driven Blockchain Design for Supply-Chain Provenance},
  author = {Kim, Henry M. and Laskowski, Marek},
  date = {2018},
  journaltitle = {Intelligent Systems in Accounting, Finance and Management},
  shortjournal = {Intell. Syst. Account. Finance Manag.},
  volume = {25},
  number = {1},
  pages = {18--27},
  issn = {1099-1174},
  doi = {10.1002/isaf.1424},
  abstract = {An interesting research problem in our age of Big Data is that of determining provenance. Granular evaluation of provenance of physical goods (e.g., tracking ingredients of a pharmaceutical or demonstrating authenticity of luxury goods) has often not been possible with today's items that are produced and transported in complex, interorganizational, often internationally spanning supply chains. Recent adoptions of the Internet of Things and blockchain technologies give promise at better supply-chain provenance. We are particularly interested in the blockchain, as many favored use cases of blockchain are for provenance tracking. We are also interested in applying ontologies, as there has been some work done on knowledge provenance, traceability, and food provenance using ontologies. In this paper, we make a case for why ontologies can contribute to blockchain design. To support this case, we analyze a traceability ontology and translate some of its representations to smart contracts that execute a provenance trace and enforce traceability constraints on the Ethereum blockchain platform.},
  langid = {english},
  keywords = {blockchain,distributed ledger,enterprise modeling,Ethereum,ontology,provenance,smart contracts,supply-chain provenance,traceability},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/isaf.1424},
  file = {/Users/brendan/Zotero/storage/QC5BJ27X/isaf.html}
}

@online{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2018-11-07},
  shortjournal = {ArXiv171209913 Cs Stat},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.09913},
  urldate = {2021-03-15},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brendan/Zotero/storage/AK4HUVS9/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf;/Users/brendan/Zotero/storage/X7DDG8ZT/1712.html}
}

@book{mahoneyAIFairnessHow2020,
  title = {{{AI}} Fairness: How to Measure and Reduce Unwanted Bias in Machine Learning},
  shorttitle = {{{AI}} Fairness},
  author = {Mahoney, Trisha and Varshney, Kush R and Hind, Michael},
  date = {2020},
  url = {http://proquest.safaribooksonline.com/?fpi=9781492077664},
  urldate = {2021-02-08},
  langid = {english},
  annotation = {OCLC: 1195889812}
}

@incollection{mcleodTrust2020,
  title = {Trust},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {McLeod, Carolyn},
  editor = {Zalta, Edward N.},
  date = {2020},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  url = {https://plato.stanford.edu/archives/fall2020/entries/trust/}
}

@incollection{miikkulainenChapter15Evolving2019,
  title = {Chapter 15 - {{Evolving Deep Neural Networks}}},
  booktitle = {Artificial {{Intelligence}} in the {{Age}} of {{Neural Networks}} and {{Brain Computing}}},
  author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
  editor = {Kozma, Robert and Alippi, Cesare and Choe, Yoonsuck and Morabito, Francesco Carlo},
  date = {2019-01-01},
  pages = {293--312},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-815480-9.00015-3},
  abstract = {The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.},
  isbn = {978-0-12-815480-9},
  langid = {english},
  keywords = {Deep Learning,Evolutionary Computation,Gated Recurrent Networks,Image Captioning,Language Modeling,Neural Architecture Search,Neural Networks,Neuroevolution,Object Recognition},
  file = {/Users/brendan/Zotero/storage/WF72L44T/Miikkulainen et al. - 2019 - Chapter 15 - Evolving Deep Neural Networks.pdf;/Users/brendan/Zotero/storage/MJ6KHQ9N/B9780128154809000153.html}
}

@inproceedings{palacioWhatDeepNetworks2018,
  title = {What Do {{Deep Networks Like}} to {{See}}?},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Palacio, Sebastian and Folz, Joachim and Hees, Jorn and Raue, Federico and Borth, Damian and Dengel, Andreas},
  date = {2018-06},
  pages = {3108--3117},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00328},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  file = {/Users/brendan/Zotero/storage/ARIF5NR2/Palacio et al. - 2018 - What do Deep Networks Like to See.pdf}
}

@report{projectsherpaHttpsWwwProjectsherpa2019,
  title = {{{https://www.project-sherpa.eu/ethics-by-design/}}},
  author = {{Project Sherpa}},
  date = {2019},
  url = {https://www.project-sherpa.eu/ethics-by-design/}
}

@article{ryanAIWeTrust2020,
  title = {In {{AI We Trust}}: {{Ethics}}, {{Artificial Intelligence}}, and {{Reliability}}},
  shorttitle = {In {{AI We Trust}}},
  author = {Ryan, Mark},
  date = {2020-10},
  journaltitle = {Science and Engineering Ethics},
  shortjournal = {Sci Eng Ethics},
  volume = {26},
  number = {5},
  pages = {2749--2767},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-020-00228-y},
  abstract = {Abstract                            One of the main difficulties in assessing artificial intelligence (AI) is the tendency for people to anthropomorphise it. This becomes particularly problematic when we attach human moral activities to AI. For example, the European Commission’s High-level Expert Group on AI (HLEG) have adopted the position that we should establish a relationship of               trust               with AI and should cultivate trustworthy AI (HLEG AI Ethics guidelines for trustworthy AI, 2019, p. 35). Trust is one of the most important and defining activities in human relationships, so proposing that AI should be trusted, is a very serious claim. This paper will show that AI cannot be something that has the capacity to be trusted according to the most prevalent definitions of trust because it does not possess emotive states or can be held responsible for their actions—requirements of the affective and normative accounts of trust. While AI meets all of the requirements of the rational account of trust, it will be shown that this is not actually a type of trust at all, but is instead, a form of reliance. Ultimately, even complex machines such as AI should not be viewed as trustworthy as this undermines the value of interpersonal trust, anthropomorphises AI, and diverts responsibility from those developing and using them.},
  langid = {english},
  file = {/Users/brendan/Zotero/storage/YGYUTUTE/Ryan - 2020 - In AI We Trust Ethics, Artificial Intelligence, a.pdf}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient}}-Based {{Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2020-02},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  eprinttype = {arxiv},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/brendan/Zotero/storage/FKANPDF5/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf;/Users/brendan/Zotero/storage/FJSTAZAH/1610.html}
}

@online{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  shortjournal = {ArXiv13126034 Cs},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2021-01-31},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/brendan/Zotero/storage/6ZCD8B8K/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf;/Users/brendan/Zotero/storage/WU4SV2SL/1312.html}
}

@online{teamKerasDocumentationVisualizing,
  title = {Keras Documentation: {{Visualizing}} What Convnets Learn},
  shorttitle = {Keras Documentation},
  author = {Team, Keras},
  url = {https://keras.io/examples/vision/visualizing_what_convnets_learn/},
  urldate = {2021-08-09},
  abstract = {Keras documentation},
  langid = {english},
  file = {/Users/brendan/Zotero/storage/FHYGJJZ3/visualizing_what_convnets_learn.html}
}

@online{teamKerasDocumentationVisualizinga,
  title = {Keras Documentation: {{Visualizing}} What Convnets Learn},
  shorttitle = {Keras Documentation},
  author = {Team, Keras},
  url = {https://keras.io/examples/vision/visualizing_what_convnets_learn/},
  urldate = {2021-08-09},
  abstract = {Keras documentation},
  langid = {english},
  keywords = {ConvNets,Visualisation},
  file = {/Users/brendan/Zotero/storage/FRPXCJB5/visualizing_what_convnets_learn.html}
}

@report{technologySecureHashStandard2015,
  title = {Secure {{Hash Standard}} ({{SHS}})},
  author = {and Technology, National Institute of Standards},
  date = {2015-08-04},
  number = {Federal Information Processing Standard (FIPS) 180-4},
  institution = {{U.S. Department of Commerce}},
  doi = {10.6028/NIST.FIPS.180-4},
  abstract = {This standard specifies hash algorithms that can be used to generate digests of messages. The digests are used to detect whether messages have been changed since the digests were generated. The Applicability Clause of this standard was revised to correspond with the release of FIPS 202, 'SHA-3 Standard: Permutation-Based Hash and Extendable-Output Functions,' which specifies the SHA-3 family of hash functions, as well as mechanisms for other cryptographic functions to be specified in the future. The revision to the Applicability Clause approves the use of hash functions specified in either FIPS 180-4 or FIPS 202 when a secure hash function is required for the protection of sensitive, unclassified information in Federal applications, including as a component within other cryptographic algorithms and protocols.},
  langid = {english},
  keywords = {sha256},
  file = {/Users/brendan/Zotero/storage/YFL5U9SC/Technology - 2015 - Secure Hash Standard (SHS).pdf;/Users/brendan/Zotero/storage/NFTAVC3M/final.html}
}

@online{zeilerVisualizingUnderstandingConvolutional2013,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  date = {2013-11-28},
  shortjournal = {ArXiv13112901 Cs},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1311.2901},
  urldate = {2021-01-31},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/brendan/Zotero/storage/PYP4JH2W/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf;/Users/brendan/Zotero/storage/SYBNT55R/1311.html}
}


