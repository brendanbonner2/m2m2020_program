
@article{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2021-01-31},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv: 1312.6034},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/brendan/Zotero/storage/WU4SV2SL/1312.html:text/html;arXiv Fulltext PDF:/Users/brendan/Zotero/storage/6ZCD8B8K/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf},
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2021-01-31},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/brendan/Zotero/storage/SYBNT55R/1311.html:text/html;arXiv Fulltext PDF:/Users/brendan/Zotero/storage/PYP4JH2W/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
}

@article{alain_understanding_2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	urldate = {2021-01-31},
	journal = {arXiv:1610.01644 [cs, stat]},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2018},
	note = {arXiv: 1610.01644},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/brendan/Zotero/storage/KSV28M9G/1610.html:text/html;arXiv Fulltext PDF:/Users/brendan/Zotero/storage/Q8F7QYXJ/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf:application/pdf},
}

@article{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	urldate = {2021-01-31},
	journal = {arXiv:1508.06576 [cs, q-bio]},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = sep,
	year = {2015},
	note = {arXiv: 1508.06576},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/brendan/Zotero/storage/LTZEM5BE/Gatys et al. - 2015 - A Neural Algorithm of Artistic Style.pdf:application/pdf;arXiv.org Snapshot:/Users/brendan/Zotero/storage/MNBXMGKT/1508.html:text/html},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2021-01-31},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv: 1610.02391},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {336--359},
	annote = {Comment: This version was published in International Journal of Computer Vision (IJCV) in 2019; A previous version of the paper was published at International Conference on Computer Vision (ICCV'17)},
	file = {arXiv Fulltext PDF:/Users/brendan/Zotero/storage/FKANPDF5/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf;arXiv.org Snapshot:/Users/brendan/Zotero/storage/FJSTAZAH/1610.html:text/html},
}

@article{bau_network_2017,
	title = {Network {Dissection}: {Quantifying} {Interpretability} of {Deep} {Visual} {Representations}},
	shorttitle = {Network {Dissection}},
	url = {http://arxiv.org/abs/1704.05796},
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	urldate = {2021-02-06},
	journal = {arXiv:1704.05796 [cs]},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.05796},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, I.2.10},
	annote = {Comment: First two authors contributed equally. Oral presentation at CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/brendan/Zotero/storage/WNNFCCH9/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf:application/pdf;arXiv.org Snapshot:/Users/brendan/Zotero/storage/XPD9ZHSX/1704.html:text/html},
}

@inproceedings{palacio_what_2018,
	address = {Salt Lake City, UT},
	title = {What do {Deep} {Networks} {Like} to {See}?},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578426/},
	doi = {10.1109/CVPR.2018.00328},
	urldate = {2021-02-06},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Palacio, Sebastian and Folz, Joachim and Hees, Jorn and Raue, Federico and Borth, Damian and Dengel, Andreas},
	month = jun,
	year = {2018},
	pages = {3108--3117},
	file = {Submitted Version:/Users/brendan/Zotero/storage/ARIF5NR2/Palacio et al. - 2018 - What do Deep Networks Like to See.pdf:application/pdf},
}

@article{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://arxiv.org/abs/1711.11279},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	urldate = {2021-02-06},
	journal = {arXiv:1711.11279 [stat]},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jun,
	year = {2018},
	note = {arXiv: 1711.11279},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/brendan/Zotero/storage/PP8222XT/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf:application/pdf;arXiv.org Snapshot:/Users/brendan/Zotero/storage/ZZBHYXX3/1711.html:text/html},
}

@book{mahoney_ai_2020,
	title = {{AI} fairness: how to measure and reduce unwanted bias in machine learning},
	shorttitle = {{AI} fairness},
	url = {http://proquest.safaribooksonline.com/?fpi=9781492077664},
	language = {English},
	urldate = {2021-02-08},
	author = {Mahoney, Trisha and Varshney, Kush R and Hind, Michael},
	year = {2020},
	note = {OCLC: 1195889812},
}

@article{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {http://arxiv.org/abs/1712.09913},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2021-03-15},
	journal = {arXiv:1712.09913 [cs, stat]},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	month = nov,
	year = {2018},
	note = {arXiv: 1712.09913},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2018 (extended version, 10.5 pages), code is available at https://github.com/tomgoldstein/loss-landscape},
	file = {arXiv Fulltext PDF:/Users/brendan/Zotero/storage/AK4HUVS9/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf:application/pdf;arXiv.org Snapshot:/Users/brendan/Zotero/storage/X7DDG8ZT/1712.html:text/html},
}

@techreport{project_sherpa_httpswwwproject-sherpaeuethics-by-design_2019,
	title = {https://www.project-sherpa.eu/ethics-by-design/},
	url = {https://www.project-sherpa.eu/ethics-by-design/},
	author = {{Project Sherpa}},
	year = {2019},
}

@techreport{high-level_expert_group_on_ai_ethics_2019,
	address = {Brussels},
	type = {Report},
	title = {Ethics guidelines for trustworthy {AI}},
	url = {https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai},
	abstract = {The aim of the Guidelines is to promote Trustworthy AI. Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them.},
	language = {eng},
	institution = {European Commission},
	author = {{High-Level Expert Group on AI}},
	month = apr,
	year = {2019},
	keywords = {European\_Commission artificial\_intelligence digital\_ethics discrimination ethics},
}

@article{ryan_ai_2020,
	title = {In {AI} {We} {Trust}: {Ethics}, {Artificial} {Intelligence}, and {Reliability}},
	volume = {26},
	issn = {1353-3452, 1471-5546},
	shorttitle = {In {AI} {We} {Trust}},
	url = {https://link.springer.com/10.1007/s11948-020-00228-y},
	doi = {10.1007/s11948-020-00228-y},
	abstract = {Abstract
            
              One of the main difficulties in assessing artificial intelligence (AI) is the tendency for people to anthropomorphise it. This becomes particularly problematic when we attach human moral activities to AI. For example, the European Commission’s High-level Expert Group on AI (HLEG) have adopted the position that we should establish a relationship of
              trust
              with AI and should cultivate trustworthy AI (HLEG AI Ethics guidelines for trustworthy AI, 2019, p. 35). Trust is one of the most important and defining activities in human relationships, so proposing that AI should be trusted, is a very serious claim. This paper will show that AI cannot be something that has the capacity to be trusted according to the most prevalent definitions of trust because it does not possess emotive states or can be held responsible for their actions—requirements of the affective and normative accounts of trust. While AI meets all of the requirements of the rational account of trust, it will be shown that this is not actually a type of trust at all, but is instead, a form of reliance. Ultimately, even complex machines such as AI should not be viewed as trustworthy as this undermines the value of interpersonal trust, anthropomorphises AI, and diverts responsibility from those developing and using them.},
	language = {en},
	number = {5},
	urldate = {2021-07-29},
	journal = {Science and Engineering Ethics},
	author = {Ryan, Mark},
	month = oct,
	year = {2020},
	pages = {2749--2767},
	file = {Full Text:/Users/brendan/Zotero/storage/YGYUTUTE/Ryan - 2020 - In AI We Trust Ethics, Artificial Intelligence, a.pdf:application/pdf},
}

@article{jobin_global_2019,
	title = {The global landscape of {AI} ethics guidelines},
	volume = {1},
	copyright = {2019 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0088-2},
	doi = {10.1038/s42256-019-0088-2},
	abstract = {In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.},
	language = {en},
	number = {9},
	urldate = {2021-07-30},
	journal = {Nature Machine Intelligence},
	author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
	month = sep,
	year = {2019},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 9
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Ethics;Information systems and information technology;Information technology;Science, technology and society
Subject\_term\_id: ethics;information-systems-and-information-technology;information-technology;science-technology-and-society},
	pages = {389--399},
	annote = {https://doi.org/10.1038/s42256-019-0088-2},
	file = {Snapshot:/Users/brendan/Zotero/storage/3SAZAAA3/s42256-019-0088-2.html:text/html;Submitted Version:/Users/brendan/Zotero/storage/BNH5DQ55/Jobin et al. - 2019 - The global landscape of AI ethics guidelines.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
	annote = {This},
	file = {IEEE Xplore Abstract Record:/Users/brendan/Zotero/storage/6JTBJMDE/5206848.html:text/html},
}

@article{asaro_ai_2019,
	title = {{AI} {Ethics} in {Predictive} {Policing}: {From} {Models} of {Threat} to an {Ethics} of {Care}},
	volume = {38},
	issn = {0278-0097, 1937-416X},
	shorttitle = {{AI} {Ethics} in {Predictive} {Policing}},
	url = {https://ieeexplore.ieee.org/document/8733937/},
	doi = {10.1109/MTS.2019.2915154},
	number = {2},
	urldate = {2021-08-09},
	journal = {IEEE Technology and Society Magazine},
	author = {Asaro, Peter M.},
	month = jun,
	year = {2019},
	pages = {40--53},
}

@article{kevin_understanding_2019,
	title = {Understanding {Ethics} and {Human} {Rights} in {Smart} {Information} {Systems}},
	volume = {2},
	issn = {25158562},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2515856220300079},
	doi = {10.29297/orbit.v2i1.102},
	language = {en},
	number = {2},
	urldate = {2021-08-09},
	journal = {The ORBIT Journal},
	author = {Kevin, Macnish and Mark, Ryan and Bernd, Stahl},
	year = {2019},
	pages = {1--34},
	file = {Full Text:/Users/brendan/Zotero/storage/2VHZXQ8W/Kevin et al. - 2019 - Understanding Ethics and Human Rights in Smart Inf.pdf:application/pdf},
}

@incollection{mcleod_trust_2020,
	title = {Trust},
	url = {https://plato.stanford.edu/archives/fall2020/entries/trust/},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {McLeod, Carolyn},
	editor = {Zalta, Edward N.},
	year = {2020},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://arxiv.org/abs/1512.03385v1},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2021-08-09},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	file = {Full Text PDF:/Users/brendan/Zotero/storage/N9WFKGC3/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:/Users/brendan/Zotero/storage/RMW4KVK3/1512.html:text/html},
}

@misc{team_keras_nodate,
	title = {Keras documentation: {Visualizing} what convnets learn},
	shorttitle = {Keras documentation},
	url = {https://keras.io/examples/vision/visualizing_what_convnets_learn/},
	abstract = {Keras documentation},
	language = {en},
	urldate = {2021-08-09},
	author = {Team, Keras},
	file = {Snapshot:/Users/brendan/Zotero/storage/FHYGJJZ3/visualizing_what_convnets_learn.html:text/html},
}

@misc{team_keras_nodate-1,
	title = {Keras documentation: {Visualizing} what convnets learn},
	shorttitle = {Keras documentation},
	url = {https://keras.io/examples/vision/visualizing_what_convnets_learn/},
	abstract = {Keras documentation},
	language = {en},
	urldate = {2021-08-09},
	author = {Team, Keras},
	keywords = {ConvNets, Visualisation},
	file = {Snapshot:/Users/brendan/Zotero/storage/FRPXCJB5/visualizing_what_convnets_learn.html:text/html},
}

@misc{gene_kogan_what_2021,
	title = {What {Neural} {Networks} {See} by {Gene} {Kogan} - {Experiments} with {Google}},
	url = {https://experiments.withgoogle.com/what-neural-nets-see},
	abstract = {Since 2009, coders have created thousands of amazing experiments using Chrome, Android, AI, WebVR, AR and more. We're showcasing projects here, along with helpful tools and resources, to inspire others to create new experiments.},
	urldate = {2021-08-09},
	author = {{Gene Kogan}},
	year = {2021},
	file = {Snapshot:/Users/brendan/Zotero/storage/ZWTT86YC/what-neural-nets-see.html:text/html},
}

@inproceedings{golle_machine_2008,
	title = {Machine learning attacks against the {Asirra} {CAPTCHA}},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security},
	author = {Golle, Philippe},
	year = {2008},
	pages = {535--542},
	file = {Snapshot:/Users/brendan/Zotero/storage/N38CZRPQ/1455770.html:text/html},
}
