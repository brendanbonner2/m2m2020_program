\section{Conclusions and Recommendations}

The conclusion of the \textit{Practicum} is there is currently no simple and efficient method of verifying the lifecycle of the development of the model. When models are released, either for production, competitions or uploaded to critical systems, they remain a blackbox. While developing a system for verifying the history of the model, it will allow us to both gain an insight into the development process, but also be able to check shared characteristics between machine learning deep learning systems.

We explored a number of novel elements to provide more information about a model that can be used to identify differences between models, and the difference between layers of models. The next phase of the research will be to develop a study based on real world use in a development environment. The practical application yielded more data than originally envisaged, particularly in relation to the transferability of the modules to all deep neural networks.

Visualisation applications were limited to graphs showcasing the difference between models. It was not necessary to explore 3D comparison primarily because the layer information was reduced to two variables. While they have the capacity to continuously update an visual family tree while the system is in training, a direct \textit{tensorboard} style application may be more beneficial .

\subsection{Recommendations}


Our experimentation into the personality of a neural network model shows that there is additional scope for further research using these finding within the scope of interpretability and explainability. Comparisons between training models error rates and the difference in layers provide an indication into how training is successful, although there was an interesting observation where some layers were trained \textit{more}, that is the similarity between layers changed out of sync with error rates.

There are several promising spillover activities that can follow this approach. The majority are related to explainability, although a more pertinent issue will be utilisation of the techniques in this paper to a practical application for real world auditing of models as they are being trained and released. In addition, the following areas could increase knowledge of uptake of explainable AI.

\begin{enumerate}
    \item 
    Increasing information stored. 
    Creating the information blocks at a more granular level to generate the checksum with all the information currently stored can provide more utility. One the other hand, this would create a non-optimal storage if we included the full set of model weights. There may be a compromise to provide more visibility, while retaining unique characteristics of the model.
    
    \item 
    Introduction of blockchain verification.
    Currently using database solutions that can be accessed publicly. Integrating a distributed ledger to store the public models may provide more security in the models. Until, it is required, the one-way hashing algorithm provide security that the model used is identical, or closely related, to a signature.
    
    \item 
    Closer integration with Machine Learning providers.
    Including push functionality into the training and deployment process, as well as the inclusion of visualisation tools, could increase uptake and provide more sources to compare.
    
    \item 
    Testing in a model distribution environment.
    The solution can be used in an environment where pre-trained of default models can be verified prior to manipulation. This can be used to establish a metric of similarity between models (the functions have been included) once trained.
\end{enumerate}


\section{Acknowledgement}

I would like to sincerely thank Dr. Alessandra Mileo for the guidance, motivation and support in developing this practicum.