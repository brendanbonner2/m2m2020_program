\section{Background}

The central tenet of the \textit{Practicum} is how to determine a methodology of trust in a digital world, where there is widely reported scepticism. The need and demand for evaluating a methodology for efficiently recording the learning process of an AI model will be researched, focusing on convolutional neural networks. The approach will establish if existing CNN explainability methods can be used to define a novel temporal chain of major differences in the underlying structure of the network during training.
 
Once we have determined what elements of value in the process show what can be recorded, it will create a verification repository for the network (similar to a \textit{Résumé} ) and show how this can be used as either a trust chain when establishing the provenance of the model, reviewing the network training in comparison with the post-network visualisation, or providing a mechanism to see if the training process is either optimal or biased.

The research area overlaps between the current drive to explain black box models to address trust issues with artificial intelligence\cite{ryanAIWeTrust2020}, alongside the drive to be able to optimise and evaluate the process for creating and expanding existing models. The bulk of current research focuses on the behaviour of neural networks \textit{post-}training, examining how an input triggers various layers. This functionality of this is reduced as deeper networks become more commonplace. Starting to record and validate how networks are trained, rather than utilised may provide additional support towards trustworthy AI as in the He paper comparing training\cite{heDeepResidualLearning2015a}, while benefiting training evaluations with visualisation of the gravity of neurons are activated during the learning process.

Models in CNNs are the primary reason for opacity of neural networks. Decision trees and algorithms have the benefit of being easily repeatable outside of the system. The complexity of even the simplest models makes this unviable. The introduction usually describes the background of the project with brief information on general knowledge of the subject. This sets the scene by stating the problem being tackled and what the aims of the project are.

\subsection{Overview: A Question of Trust}

In his overview of ethics in AI, Ryan \cite{ryanAIWeTrust2020} identified that from a high level, trust is straightforward - artificial systems should \textbf{not} be trusted - the background to this is the criteria for trust has not been met, namely that we cannot be vulnerable to these systems, the system thinks well in others and be optimistic that the system is competent\cite{mcleodTrust2020}. Some machine learning techniques, although very successful from the accuracy point of view, are very opaque in terms of understanding how they make decisions. The notion of black-box AI refers to such scenarios, where it is not possible to trace back to the reason for certain decisions. Explainability is a property of those AI systems that instead can provide a form of explanation for their actions, without fully understanding how the system came to this outcome.

\subsection{Linking Software Models with Trust}
The link between achieving trust, and development of software models in deep neural networks, are the basis for the fuzzy nature of a certain outcome based on an input. As the input changes, the impact on weights, balances and activation functions in trained models can provide significant changes in outcomes, whether we a are classifying the image, identifying components or using the outcome as the basis for generating an outcome\cite{miikkulainenChapter15Evolving2019}. In frameworks like Keras and PyTorch, the Model contains roughly the same structure, based on an input layer, a number of hidden layers, and finally a classification or output layer. As a model is trained, the input layer and all not weighted layers remain the same, but the back propagation function continuously changes the weights and biases across multiple layers with diverse interlinks\cite{hecht-nielsenIIITheoryBackpropagation1992}. Trusting a system developed in this manner would require an understanding of how every input caused a reaction, or else if it was possible to describe every pathway with a reasonable accuracy. In systems with over 1.6 trillion interacting parameters\cite{fedusSwitchTransformersScaling2021}, this is not going to be viable, alternatives will be needed.

\subsection{Identifying Differences in Models}
To reach develop this further, we needed to investigate what changes when a model is altered. Each layer is made up of a structure containing a dictionary of structural, compilation and information details, plus a multi dimensional array containing the weights for each sublayer that covers every  dimensions, plus a similar series of biases. While trying to understanding the internals of layers is interesting, but relatively futile, being able to break down the structure of the layer into a unique single identifier that is non-repeatable across the smallest changes is interesting. This also allows comparison between instances of the layers when trained.

What is not provided for is the need and demand for evaluating the development \& methodology for efficiently recording the learning process of an AI model, focusing on convolutional neural networks. Can current CNN explainability methods highlight a temporal chain of major differences in the underlying structure of the network during training. Existing research into provenance of electronic records is within the domain of blockchain technology, usually in a transactional process\cite{kimOntologydrivenBlockchainDesign2018}. create a verification repository for the network and show how this can be used as either a trust chain when for establishing the provenance of the model, reviewing the network training in comparison with the post-network visualisation, or providing a mechanism to see if the training process is either optimal or biased.

The research area overlaps between the current drive tin explainable AI, with the drive to be able to optimise and evaluate the process for creating and expanding existing models. The bulk of current research focuses on the behaviour of neural networks after training, examining how an input triggers various layers, and this has reduced as deeper networks become more commonplace. Starting to record and validate how networks are trained, rather than used may provide additional support to trustworthy AI, while benefiting training evaluations and visualisation of when neurons are activated during the learning process.


\subsection{Current Research into Trustworthy AI}
The majority of Trustworthy AI research eventually leads to one outcome; risk levels of the application or visualisation of the system when making a decision. As this is moving from software to social engineering, this lean is veering more towards a systemic classification of risk levels for AI systems. This is popularised by both practitioners from within, and also regulatory and political channels.

In the process of understanding and trusting deep neural networks, researchers have veered towards extensive evaluation to identify neural pathway activation, or by presenting input triggers and looking at activations between layers\cite{liVisualizingLossLandscape2018}. There is a difference in approach regarding what has generated utility in explainability - allowing multiple metrics to cover the effect of individual features, morphology characteristics or how pixels resonate various components of the network. The focus is to understand the network, rather than understand how it is trained. The closest research in understanding the training process is the use of classifier probes, and how these can be recorded in a provenance chain.

How do we start evaluating the trustworthiness of AI systems. This is achieved by examining how fairness and bias is interpreted across all artificial intelligence systems. In \textit{AI fairness: how to measure and reduce unwanted bias in machine learning}\cite{mahoneyAIFairnessHow2020}, the research examines each phase of the AI deployment process to identify where bias is created, identified and countered. The resultant research created the IBM AI Fairness (AIF360) toolkit, which looks at all network types and rewires the network weights and biases to remove predefined bias from the outcome. This works well on business AI systems classifiers like random forest and naive bayes, and very shallow neural networks, but is highly limited in deep neural networks. The research does identify one important gap - there is no mechanism to alter a trained network, and expect it to behave as before. Part of the research will provide a trace to verify that the network being examined is mathematically verified to not be tampered with, potentially increasing the trust in deployments.

This research stops short once we start working with convolutional neural networks and transformers, due to the increasing depth of networks. The understanding of the internal activations within \textit{ConvNets} was discussed in the 2014 paper Deep Inside Convolutional Networks \cite{simonyanDeepConvolutionalNetworks2014}. These internals changed when presented with an image to be classified which was visualised using saliency maps. The method of saliency extraction with a view to presenting a single 2D image map gave a “last layer” backwards view of the network activations, as if you were looking through a telescope, while imagining the neurons. This research highlighted that aggregation of individual neuron activations can be utilised for explaining CNNs. If we could turn the view around, it might be possible that instead of seeing the activations of neurons, we can see the neurons being trained.

Other papers focused on making a score of interpretability by breaking down the components are discussed in \textit{Network Dissection: Quantifying Interpretability of Deep Visual Representations}\cite{bauNetworkDissectionQuantifying2017}. Bau et al, introduces the concept of scoring unit interpretability to give a segmentation threshold to an individual concept when the CNN is classifying. The objective is to visualise and score diverse CNNs to evaluate discrimination and interpretability. Would it be possible to make a link between the number of detectors in the outcome tests to the number of times a certain portion of the network is trained to classify or ignore that concept. If this is not viable, would an alternative be to see the direction of training.


\subsection{Interpretation of the Learning Process}

Once we understand that there is a gap in being able to visualise the learning process when training each convolutional neural pathway in an AI system, what is available. Research is quite light, as the focus has been on understanding a network when it is making a classification or using evaluation to interpret differences in classification. We need an alternative approach to examine how individual elements of each layer are tweaked during the training process. Brute force approaches would be to record the loss function changes in back-propagation for every trained step, this would result in an exponentially increasing compute requirement, which would require multi-terabyte storage for even a competition trained VGG-16 network. Any approach has to be optimal to record only changes in neurons that are beyond an average activation threshold, and then only in a constant dimension depth per layer. The resulting delta will be configured every epochs to provide a consistent path of evolution during training.

One paper looking at \textit{Understanding intermediate layers using linear classifier probes}\cite{alainUnderstandingIntermediateLayers2018} provide an appreciation of the value of examining the learning process as opposed to the post-trained model activations. Although still concerned with feature activation, it introduces a new method that sits in parallel to the traditional neural network library of tools. In this case, monitors of features at every layer of a model are evaluates suitability for classification. The approach is to have completely independent linear classifiers, which is needed for delivery of this theme. This was applied to two large models in Inception v3 and Resnet-50, and the resulting analysis of similar networks helped us focus on high performing and large networks like VGG-16. Any solution will need to manage these sizes, and be useful to provides reverse input into this research for optimising the training for visualisation.
    
This concept was expanded by identifying how low level concepts interact with high level features in \textit{Quantitative Testing with Concept Activation Vectors}\cite{kimInterpretabilityFeatureAttribution2018}. This confirms that explainability remains a significant challenge in the AI world, while still important to attempt to de-opaque to make sure that a value based machine learning world is possible. The paper uses CAVs for interpretability for testing, and the authors practical application of proposed extensions to TCAVs shows a link between the learning pattern changes with the shape, texture of repetition of the training images.  The findings were also used to develop artistic style generative images based on the test outcomes. While we will not attempt to replicate the Empirical Deepdream, visualisation of the outcomes should be available. While aesthetic to view, the outcome always remains dependant on their inputs.

Beyond this, other visualisation techniques such as the Stuttgart Neural Network Simulator (SNNS),allows the early definition and visualisation (based on X11 graphics). A good example of research is Adam Harley's visualisation tool\cite{bebisAdvancesVisualComputing2015} that shows how to visualise a full CNN complete with all interconnections, and weighted values that are activated on (in this case MNIST) data and a hand drawn input.

\subsection{Future of Trustworthy AI research}
While there has been continuous research and improvement into the field of explainability, there remains a disconnect between the understanding of why a neural network produces a specific outcome – of which research is plentiful; and a lack of investigation into establishing a trust quotient to provide verifiable classifications based on either its initial training or amendments provided after transfer. The benefit of the research is there is a clear metric and methodology to verify what components of a CNN are activated by a source image, and we can do this not only in an interesting way that can show how far down a deep network the feature or concept is important, but also to guide optimisations. CNN visualisation is also strong, but based on it's use not how it was trained.

What is lacking is a guidance as to why the training of the network led to this behaviour. If we follow the introductory analogy of a human being required to make a classification, and looking at their background, their education, their environment for the acquired competence – nearly all of this is used to build a body of evidence that this experience can be trusted to make a decision or classification. Like lifelong learning being recorded in a verifiable resumé. CNNs are trained on a repository of either a single domain, or pre-trained using a generic corpus of data, and while the outcomes may be similar, an anomaly in training or a tweak afterwards will yield untrustworthy results, which can be evaluated only extensive measurements.

The missing link is being able to see the training process, similar to recording inputs that a human will see from birth to deployment in the workplace. While we do not need to see everything, we would benefit from seeing the important triggers over time that form and tweak the neuron. This is the aim - to record and store the life-log of the network, to see what parts of the training process were not necessary and to optimise before an exponentially wasteful under-fitting of pathways becomes an unnecessary overhead in deployment. What is also missing is being able to baseline this model alongside this record, which would give a provenance. In the event of the next generation of fairness manipulation system, removing bias for any reason like Giskard from Asimov’s Robots of Dawn, provenance shows if that manipulation was performed, and If the model can be trusted.
