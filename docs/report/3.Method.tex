\section{Method}
Method should outline how the task/experiment was carried out, including rationale for any decisions made. Details of any equipment and subjects used should be also included. Basically, you should include enough information, so that the reader could duplicate as much of the experimental conditions or design details as possible.

\subsection{Reducing Model without Losing Character}
The first part of the delivery required that a model, in this case a CNN model in Keras, could be reduced to an information block that each layer contains a summary of the layer that can record any changes in the weights and biases. The structure of a model is an 
\begin{enumerate}
\item input shape
\item series of dense layers
\item output layers
\end{enumerate}

and within each of the layers, there are is a series of layers that, depending on type, will contain a series of tuples that have a single of multi-dimensional series of weights, plus a set of biases. it is within these layers that we will extract the information. As there is potentially a lot of information in the layer, we attempted to reduce this by extracting two identifying elements

Skew:
\[ G_1=\frac{k_3}{k_2^{3/2}}=\frac{\sqrt{N(N-1)}}{N-2}\frac{m_3}{m_2^{3/2}}. \]

Standard Deviation of N Numbers
\[ \frac1N\sum_{i=1}^n\left(x^i-\overline x\right)^2 \]

\begin{table}[h]
    \caption{Values generated from evaluated model}
    \setlength\tabcolsep{0pt} % make LaTeX figure out intercolumn spacing
    \begin{tabular}{@{} |p{2cm}|p{6.5cm}| @{}}
        Mean    & The average value of all layers across all dimension \\ \hline 
        StdDev  & An indication of the deviation of values, to assist in seeing how the values are weighted \\ \hline
        Histogram & A Histogram of the distribution of values across the layer. This will assist in giving a more informed distribution. As there are a large number of potential entries in the dense layers, from binary to minute changes in float values, the Histogram will be split between the values smallest non-zero granularity and the largest value across 10 bins. \\ \hline
        Name.   & Name of Layer \\ \hline
        Angle of Data & This is a single value representing the weighting of a Hash of the Layer Values (weights and biases) \\ 
    \end{tabular}
\end{table}

\subsection{Recording Changes}
As we now have a method of identifying the model in a broken down format, the next stage is to record and apply a signature to the model. There are a number of methods of ensuring that the model is recorded. The key part is to take the series of hashes on each layer, combined with the model shape, and generate a signature. The signature is what will be recorded as an immutable value that can be regenerated every time the \textbf{exact} same model is used. With this identification, we can create a list showing the parent and child of the model.

To ensure this is done correctly, we have two applications - local and global.
\subsection{Global Changes}
For global changes there will be a globa repository containing the signature and summary of published models. 

\begin{table}[h]
\caption{Model Reference Storage}

\begin{tabular}{| p{0.25\linewidth} | p{0.7\linewidth} |}
    \hline
    Model Identifier    & This is the checksum generated by the model \\
    \hline
    Creation Date       & This will be the timestamp that the model was first added to the repository \\
    \hline
    Parent              & The model identification of the direct parent of the model. \\
    \hline
    Model Summary       & A binary object that contains the items generated in the table above. \\
    \hline 
    
\end{tabular}
\end{table}

\subsection{Local Changes}

An additional benefit of the summarisation is to be able to view the model as it is being trained. This is a local filestore or database that contains a more regular record of the the summary changes. It is recommended that global model reporistory is only used for publishing models that require tracability, and not for regular changes.

In keeping with Git vocabulary, local changes are updated with the \verb|model_add(model, [parent])| command, which takes a model and returns a signature and commits the information to a local repository.

The local repository stores the abstracted model information set that can be used for local comparisons.
To commit the model to the public repository, the \verb|model_add(model, [parent])| command is used, with this time the original model that already exists in the repository. The command will return an error if the parent does not exist.


To provide reference storage for the model repositories, we created two mongodb instances with the same collection structure. There are two collections - the signatures and the model data. This was to enable verification of the evolution path and when necessary pull the model synopsis for comparison.

\texttt{
    db.model.createIndex( {signature:1},{unique:true} )
    db.model.insertOne( { signature:'0x123412341234', creator:'brendanboner@gmail.com', previous:null, date:'01-01-2020 16:25' } )
}

\subsection{Comparing Models}
To ensure there is tracibility in between the models, if a parent model is defined, a \textit{b-score} is generated with a degree of difference between the models. The score is based on two factors, the difference between the model structure, and the difference between the two primary parameters for each layer: \textit{StdDev} \& \textit{Angle of Array}

\subsection{Collision Management}

{ \itshape
In computer science, a collision or clash is a situation that occurs when two distinct pieces of data have the same hash value, checksum, fingerprint, or cryptographic digest.[1]

Due to the possible applications of hash functions in data management and computer security (in particular, cryptographic hash functions), collision avoidance has become a fundamental topic in computer science.

Collisions are unavoidable whenever members of a very large set (such as all possible person names, or all possible computer files) are mapped to a relatively short bit string. This is merely an instance of the pigeonhole principle.[1]

The impact of collisions depends on the application. When hash functions and fingerprints are used to identify similar data, such as homologous DNA sequences or similar audio files, the functions are designed so as to maximize the probability of collision between distinct but similar data, using techniques like locality-sensitive hashing.[2] Checksums, on the other hand, are designed to minimize the probability of collisions between similar inputs, without regard for collisions between very different inputs.[3]
}
