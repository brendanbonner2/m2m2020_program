{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Create a CNN for Training\n",
    "\n",
    "The images are broken down into 4 classes with the directory structure \n",
    "\n",
    "imagenett_4class/[test|train|validation]/[church|garbage_truck|gas_pump|parachute]/ILSRVC2012_val_000XXXXX.jpg\n",
    "\n",
    "The images are all 426x320x3 (RGB), broken down into:\n",
    "\n",
    "* training images [1200]\n",
    "* validation images [100]\n",
    "* test images [50]\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "The network will be compiled, and then we will use the callback functions to generate a continuous feed of updates based on the network training.\n",
    "\n",
    "As the model has 2+M parameters, there needs to be a method of classifying clusters of training that doesn't require every details to be extracted.\n",
    "\n",
    "To verify the baseline, we will use an indicator of the complete model (in hd5 format), then on a layer by layer analysis, we will compute the current checksum of each layer.\n",
    "\n",
    "The next step is aggregating the differences in the layers during each batch/step or time variation.\n",
    "\n",
    "Options:\n",
    "* reducing the layers to 2D - either summation of changes of integral\n",
    "* clustering to a smaller subset of i.e. 100x100xdimesions.\n",
    "* recording each convolution layer as a delta, easier to visualise\n",
    "\n"
   ],
   "metadata": {
    "id": "rMu_WV_ThuDh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#import all libraries needed\n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "jem81EOPI5Xx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Multiclass Image Classification\n",
    "\n",
    "The images are broken down into 4 classes with the directory structure \n",
    "\n",
    "imagenett_4class/[test|train|validation]/[church|garbage_truck|gas_pump|parachute]/ILSRVC2012_val_000XXXXX.jpg\n",
    "\n",
    "The images are all 426x320x3 (RGB), broken down into:\n",
    "\n",
    "* training images [1200]\n",
    "* validation images [100]\n",
    "* test images [50]\n"
   ],
   "metadata": {
    "id": "rMu_WV_ThuDh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Inputation\n",
    "\n",
    "identify the train:test:validation datasets based on the sample data. Use the keras ImageDataGenerator to pull in the data from the directories and use the directory name as the classifier. Fortunately, this is how the input dataset is used"
   ],
   "metadata": {
    "id": "f9-RtKaLeewn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Switch to Image Data Source Location (or gDrive)\n",
    "\n",
    "trainDataImages = ImageDataGenerator()\n",
    "testDataImages = ImageDataGenerator()\n",
    "validDataImages = ImageDataGenerator()\n",
    "\n",
    "# Load Train/Validation/Test input files from directory structure with classifications\n",
    "# Image size if 426x320, or resize to this if not correct size\n",
    "traindata = trainDataImages.flow_from_directory(directory=\"train\",target_size=(426,320), class_mode=\"categorical\")\n",
    "validdata = ValidDataImages.flow_from_directory(directory=\"validation\", target_size=(426,320),class_mode=\"categorical\")\n",
    "testdata = testDataImages.flow_from_directory(directory=\"test\", target_size=(426,320), class_mode=\"categorical\")\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1177d4020cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load Train/Validation/Test input files from directory structure with classifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Image size if 426x320, or resize to this if not correct size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtraindata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m426\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m426\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValidData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m426\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   def flow_from_dataframe(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train'"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "e4nQFdZqKMKY",
    "outputId": "8daf45bd-89e7-498a-ffd6-195a272facad"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate a sequential model, based on the VGG-16 Lite variant for the assignment\n",
    "\n",
    "#Setup the Xavier Initialiser - this is the default, but well include as explicit\n",
    "initializer = GlorotUniform()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(426,320,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\", kernel_initializer=initializer))\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\",kernel_initializer=initializer))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\",kernel_initializer=initializer))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\",kernel_initializer=initializer))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512,activation=\"relu\",kernel_initializer=initializer))\n",
    "model.add(Dense(units=4, activation=\"softmax\",kernel_initializer=initializer))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "_RkwPSPUKhlw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "x89KMG9Vj9vi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 426, 320, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 426, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 213, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 213, 160, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 213, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 106, 80, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 542720)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               277873152 \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 277,940,772\n",
      "Trainable params: 277,940,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f584999f6d8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 21
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCBiVGjJMKk5",
    "outputId": "e77d7c2f-491a-4bcb-8de6-954f8005d8be"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy','loss'])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 426, 320, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 426, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 213, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 213, 160, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 213, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 106, 80, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 542720)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               277873152 \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 277,940,772\n",
      "Trainable params: 277,940,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hSkvvlQML2R",
    "outputId": "e32391e1-aea8-453a-f396-694545049905"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation Metrics\n",
    "\n",
    "* Required Diagrams: Illustrate you answer with appropriate training and validation accuracy and loss diagrams.\n",
    "* Required Metrics: Final validation accuracy and loss, final test accuracy and loss, confusion matrix and the networks computational cost.\n",
    "* Required Models: All models developed should be saved in Hierarchical Data Format (HDF5) (.h5) format.\n",
    "\n",
    "\n",
    "Include in the metrics:\n",
    "* validation accuracy and loss\n",
    "* test accuracy and loss\n",
    "* confusion matrix\n",
    "* network computational cost\n"
   ],
   "metadata": {
    "id": "QgUFLhhRhBdf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# During training, we want to introduce a checkpoint callback (which we also use for Practicum research!)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "# Use the fit generator to take the input from the above image data generators for classification\n",
    "# Check Steps per epoch to \n",
    "hist = model.fit_generator(steps_per_epoch=1200,generator=traindata, validation_data= validdata, validation_steps=12,epochs=5,callbacks=[checkpoint,early])"
   ],
   "outputs": [],
   "metadata": {
    "id": "YrJW_W6RmeTN"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EE524 - Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "90948722effb0dbfc6ba9cb222e51e19b083ecb130b6edbb8e1b374c6a2c8c07"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}